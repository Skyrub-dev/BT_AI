{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Skyrub-dev/BT_AI/blob/main/BT_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITl913RrtYZL"
      },
      "source": [
        "# **L6 End-of-year project**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RKzqSqO5qgV"
      },
      "source": [
        "**Import Files**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "z4hrSFDsGduZ",
        "outputId": "316d7094-df05-4958-a33c-cda7530d3789"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-272d4cbdf5cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mnormed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must be in a directory that exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m   \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_signal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGKILL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must be in a directory that exists"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YVuh3Om96VN",
        "outputId": "ee3414c0-f971-4f85-f438-22d2fa333fd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9XM_FtFdp8_"
      },
      "source": [
        "**Alternatively, through a local machine**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tYtdUe3Rd2C8"
      },
      "outputs": [],
      "source": [
        "local_dir = '../Archive'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU8Icm6DY3gV"
      },
      "source": [
        "Before proceeding, check that the directory can be found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2BFYXMEKY3gV",
        "outputId": "03386797-d61b-4573-ce74-99ae0ca91a50"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Testing', 'Training', 'Validation']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(local_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAHBNGLj5_tt"
      },
      "source": [
        "**Importing libraries**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z80tFMw8eF2G"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import cv2 #checkout open cv\n",
        "import imghdr #allows to check file extensions for images\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#import kaggle - will likely change\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import layers\n",
        "from tensorflow.python.keras.models import sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlEfSL8f2Eok"
      },
      "source": [
        "GPU check optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dQ74ovch2GCk"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja8EYaeISnEg"
      },
      "source": [
        "CPU / GPU debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6wztOusSuA1",
        "outputId": "7942747d-d2ac-45e5-cf7d-599567e10df8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices(device_type=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xwK85HmsRaT"
      },
      "source": [
        "**If running off a local machine, this line of code must be run to install all dependencies**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i23KPhcisdD7",
        "outputId": "2de4c2d6-313c-4cd3-a1df-8fb56fc0c5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Collecting tensorflow-gpu\n",
            "  Downloading tensorflow_gpu-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "  Downloading tensorflow_gpu-2.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_gpu-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow_gpu-2.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.9.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow-gpu opencv-python matplotlib #! || % at the beginning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN60NljZq1_c"
      },
      "source": [
        "**Importing and loading the data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6XRz8vUswae"
      },
      "source": [
        "To import our data, we're going to be using an approach known as \"Supervised learning\". This is defined through pre-labelled datasets which the AI will be trained on. This is the best approach to solve a classification problem as the model can adjust itself and learn from its mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrsCD9TS26Lt"
      },
      "source": [
        "https://www.youtube.com/watch?v=_L2uYfVV48I\n",
        "\n",
        "This is something that you do only for toy datasets or small sample sets. In the reality using image data generator is not recommended when you have huge volumes of images or video frames (which is usually the case). In those cases is a good practice to load and split the data across multiple tfrecords, then loading it progressively with tf.data and perform multithreaded augmentations dynamically during training. The majority of the augmentations can be computed using low level tf or common python functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eqHD8gInq9Lo"
      },
      "outputs": [],
      "source": [
        "labels = ['giloma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH9nl4esw2Ws"
      },
      "source": [
        "**Loading the images**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEQV_i18Y3gh"
      },
      "source": [
        "**When using Colab, run the following lines of code from Google drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLf1DwtKY3gi"
      },
      "outputs": [],
      "source": [
        "testing_path = '/content/drive/MyDrive/data/archive/Testing'\n",
        "training_path = '/content/drive/MyDrive/data/archive/Training'\n",
        "validation_path = '/content/drive/MyDrive/data/archive/Validation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO4Euhf0Y3gj"
      },
      "source": [
        "**If using the downloaded local file, run these lines of code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bRg9HgRGY3gj"
      },
      "outputs": [],
      "source": [
        "testing_path = '../Archive/Testing/'\n",
        "training_path = '../Archive/Training/'\n",
        "validation_path = '../Archive/Validation/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0vWjgUDZXox"
      },
      "source": [
        "Do **NOT** run them both, this will cause errors, only run the one which is applicable to you"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting data into testing, training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 394 files belonging to 4 classes.\n",
            "Found 2870 files belonging to 4 classes.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The `class_names` passed did not match the names of the subdirectories of the target directory. Expected: [], but received: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [9], line 21\u001b[0m\n\u001b[0;32m      1\u001b[0m testgen \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m      2\u001b[0m     directory \u001b[39m=\u001b[39m testing_path,\n\u001b[0;32m      3\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39minferred\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m traingen \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     12\u001b[0m     directory \u001b[39m=\u001b[39m training_path,\n\u001b[0;32m     13\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39minferred\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m validgen \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     22\u001b[0m     directory \u001b[39m=\u001b[39m validation_path,\n\u001b[0;32m     23\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39minferred\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     24\u001b[0m     label_mode\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcategorical\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     25\u001b[0m     class_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mglioma_tumor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmeningioma_tumor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mno_tumor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpituitary_tumor\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     26\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m,\n\u001b[0;32m     27\u001b[0m     image_size \u001b[39m=\u001b[39m (\u001b[39m150\u001b[39m, \u001b[39m150\u001b[39m),\n\u001b[0;32m     28\u001b[0m     shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     29\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\tomke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\image_dataset.py:207\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mif\u001b[39;00m seed \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     seed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m1e6\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m image_paths, labels, class_names \u001b[39m=\u001b[39m dataset_utils\u001b[39m.\u001b[39;49mindex_directory(\n\u001b[0;32m    208\u001b[0m     directory,\n\u001b[0;32m    209\u001b[0m     labels,\n\u001b[0;32m    210\u001b[0m     formats\u001b[39m=\u001b[39;49mALLOWLIST_FORMATS,\n\u001b[0;32m    211\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[0;32m    212\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    213\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[0;32m    214\u001b[0m     follow_links\u001b[39m=\u001b[39;49mfollow_links,\n\u001b[0;32m    215\u001b[0m )\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m label_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(class_names) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWhen passing `label_mode=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`, there must be exactly 2 \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    220\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclass_names. Received: class_names=\u001b[39m\u001b[39m{\u001b[39;00mclass_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\tomke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\dataset_utils.py:533\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(class_names) \u001b[39m!=\u001b[39m \u001b[39mset\u001b[39m(subdirs):\n\u001b[1;32m--> 533\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    534\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe `class_names` passed did not match the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mnames of the subdirectories of the target directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00msubdirs\u001b[39m}\u001b[39;00m\u001b[39m, but received: \u001b[39m\u001b[39m{\u001b[39;00mclass_names\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    537\u001b[0m             )\n\u001b[0;32m    538\u001b[0m class_indices \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(class_names, \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(class_names))))\n\u001b[0;32m    540\u001b[0m \u001b[39m# Build an index of the files\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m# in the different class subfolders.\u001b[39;00m\n",
            "\u001b[1;31mValueError\u001b[0m: The `class_names` passed did not match the names of the subdirectories of the target directory. Expected: [], but received: ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']"
          ]
        }
      ],
      "source": [
        "testgen = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = testing_path,\n",
        "    labels = 'inferred',\n",
        "    label_mode= 'categorical',\n",
        "    class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
        "    batch_size = 32,\n",
        "    image_size = (150, 150),\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "traingen = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = training_path,\n",
        "    labels = 'inferred',\n",
        "    label_mode= 'categorical',\n",
        "    class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
        "    batch_size = 32,\n",
        "    image_size = (150, 150),\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "validgen = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory = validation_path,\n",
        "    labels = 'inferred',\n",
        "    label_mode= 'categorical',\n",
        "    class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'],\n",
        "    batch_size = 32,\n",
        "    image_size = (150, 150),\n",
        "    shuffle = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL9s954SlLvg",
        "outputId": "57c9462e-71da-4b91-b60c-afd876f650f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 294 images belonging to 4 classes.\n",
            "Found 2044 images belonging to 4 classes.\n",
            "Found 0 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "#https://studymachinelearning.com/keras-imagedatagenerator-with-flow_from_directory/\n",
        "\n",
        "img_size = 150 #Likely not necessary but could be used\n",
        "\n",
        "classes = 4\n",
        "#Normalising pixel values to be between 0 and 1\n",
        "testing_data_generator = ImageDataGenerator(rescale = 1./255)\n",
        "training_data_generator = ImageDataGenerator(rescale = 1./255)\n",
        "validation_data_generator = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "testing_generator = testing_data_generator.flow_from_directory (testing_path,\n",
        "                                                                target_size = (150, 150),\n",
        "                                                                color_mode = 'rgb',\n",
        "                                                                save_format = 'jpg',\n",
        "                                                                batch_size = 32,\n",
        "                                                                class_mode = 'categorical',\n",
        "                                                                classes = ['giloma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'])\n",
        "\n",
        "#Need to check these two and make sure they're configured correctly\n",
        "\n",
        "training_generator = training_data_generator.flow_from_directory (training_path,\n",
        "                                                                target_size = (150, 150),\n",
        "                                                                color_mode = 'rgb',\n",
        "                                                                save_format = 'jpg',\n",
        "                                                                batch_size = 32,\n",
        "                                                                class_mode = 'categorical',\n",
        "                                                                shuffle = True, #Prevents overfitting but when evaluating model keep the data in its original order\n",
        "                                                                subset = 'training',\n",
        "                                                                #validation_split = 0.2,\n",
        "                                                                classes = ['giloma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'])\n",
        "\n",
        "validation_generator = testing_data_generator.flow_from_directory (validation_path,\n",
        "                                                                target_size = (150, 150),\n",
        "                                                                color_mode = 'rgb',\n",
        "                                                                save_format = 'jpg',\n",
        "                                                                batch_size = 32,\n",
        "                                                                class_mode = 'categorical',\n",
        "                                                                shuffle = True,\n",
        "                                                                subset = 'validation',\n",
        "                                                                classes = ['giloma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for images, labels in testgen.batch(batch_size = 32):\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'scipy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[39m=\u001b[39m testing_generator\u001b[39m.\u001b[39mnext()\n",
            "File \u001b[1;32mc:\\Users\\tomke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py:168\u001b[0m, in \u001b[0;36mIterator.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    165\u001b[0m     index_array \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex_generator)\n\u001b[0;32m    166\u001b[0m \u001b[39m# The transformation of images is not under thread lock\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39m# so it can be done in parallel\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_batches_of_transformed_samples(index_array)\n",
            "File \u001b[1;32mc:\\Users\\tomke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py:384\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator:\n\u001b[0;32m    383\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mget_random_transform(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m--> 384\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_data_generator\u001b[39m.\u001b[39;49mapply_transform(x, params)\n\u001b[0;32m    385\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_data_generator\u001b[39m.\u001b[39mstandardize(x)\n\u001b[0;32m    386\u001b[0m batch_x[i] \u001b[39m=\u001b[39m x\n",
            "File \u001b[1;32mc:\\Users\\tomke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py:2013\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[1;34m(self, x, transform_parameters)\u001b[0m\n\u001b[0;32m   2010\u001b[0m img_col_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2011\u001b[0m img_channel_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 2013\u001b[0m x \u001b[39m=\u001b[39m apply_affine_transform(\n\u001b[0;32m   2014\u001b[0m     x,\n\u001b[0;32m   2015\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtheta\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2016\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2017\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mty\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2018\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mshear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m   2019\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mzx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[0;32m   2020\u001b[0m     transform_parameters\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mzy\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[0;32m   2021\u001b[0m     row_axis\u001b[39m=\u001b[39;49mimg_row_axis,\n\u001b[0;32m   2022\u001b[0m     col_axis\u001b[39m=\u001b[39;49mimg_col_axis,\n\u001b[0;32m   2023\u001b[0m     channel_axis\u001b[39m=\u001b[39;49mimg_channel_axis,\n\u001b[0;32m   2024\u001b[0m     fill_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfill_mode,\n\u001b[0;32m   2025\u001b[0m     cval\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcval,\n\u001b[0;32m   2026\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation_order,\n\u001b[0;32m   2027\u001b[0m )\n\u001b[0;32m   2029\u001b[0m \u001b[39mif\u001b[39;00m transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2030\u001b[0m     x \u001b[39m=\u001b[39m apply_channel_shift(\n\u001b[0;32m   2031\u001b[0m         x,\n\u001b[0;32m   2032\u001b[0m         transform_parameters[\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   2033\u001b[0m         img_channel_axis,\n\u001b[0;32m   2034\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\tomke\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\preprocessing\\image.py:2529\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[1;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[0;32m   2485\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.preprocessing.image.apply_affine_transform\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_affine_transform\u001b[39m(\n\u001b[0;32m   2487\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2499\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   2500\u001b[0m ):\n\u001b[0;32m   2501\u001b[0m     \u001b[39m\"\"\"Applies an affine transformation specified by the parameters given.\u001b[39;00m\n\u001b[0;32m   2502\u001b[0m \n\u001b[0;32m   2503\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2527\u001b[0m \u001b[39m        ImportError: if SciPy is not available.\u001b[39;00m\n\u001b[0;32m   2528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2529\u001b[0m     \u001b[39mif\u001b[39;00m scipy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2530\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   2531\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mImage transformations require SciPy. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInstall SciPy.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2532\u001b[0m         )\n\u001b[0;32m   2534\u001b[0m     \u001b[39m# Input sanity checks:\u001b[39;00m\n\u001b[0;32m   2535\u001b[0m     \u001b[39m# 1. x must 2D image with one or more channels (i.e., a 3D tensor)\u001b[39;00m\n\u001b[0;32m   2536\u001b[0m     \u001b[39m# 2. channels must be either first or last dimension\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"
          ]
        }
      ],
      "source": [
        "x = testing_generator.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing_path[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'testing_data_generator' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(testing_data_generator, num_classes\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muint8\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'testing_data_generator' is not defined"
          ]
        }
      ],
      "source": [
        "tf.keras.utils.to_categorical(testing_data_generator, num_classes=4, dtype=\"uint8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh1E02Cjwxqo"
      },
      "source": [
        "Test code below, can delete or use as backup if things go wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "MkCyI6yJiQno",
        "outputId": "951291b9-764b-4d18-fe06-6c625b684970"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-99c3932628ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m \u001b[0;31m#normalisation if above chunk of code works, will likely not need to apply this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
          ]
        }
      ],
      "source": [
        "training = []\n",
        "training_label = []\n",
        "\n",
        "testing = []\n",
        "testing_label = []\n",
        "\n",
        "#test_path = os.listdir('/content/drive/MyDrive/data/archive/Training')\n",
        "\n",
        "for i in testing_path:\n",
        "  image = cv2.imread(testing_path, cv2.IMREAD_COLOR)\n",
        "  image = cv2.resize(image, (150, 150))\n",
        "  image = image / 255.0 #normalisation if above chunk of code works, will likely not need to apply this\n",
        "  testing.append(image)\n",
        "  testing_label.append(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBhvVA5Tw6gf"
      },
      "source": [
        "Next I'll likely use Keras’ ImageDataGenerator function to augment the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbpxOKv9xCtJ"
      },
      "outputs": [],
      "source": [
        "#Example: https://studymachinelearning.com/keras-imagedatagenerator-with-flow_from_directory/\n",
        "#src_path_train = \"data/train/\"\n",
        "#src_path_test = \"data/test/\"\n",
        "\n",
        "#adjust this function\n",
        "\n",
        "data_agument = ImageDataGenerator(\n",
        "    rotation_range = 20,\n",
        "    zoom_range = 0.05,\n",
        "    width_shift_range = 0.05,\n",
        "    height_shift_range = 0.05,\n",
        "    shear_range = 0.05,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest',\n",
        "    validation_split = 0.20\n",
        ")\n",
        "\n",
        "#train_datagen = ImageDataGenerator(\n",
        "        #rescale=1 / 255.0,\n",
        "        #rotation_range=20,\n",
        "        #zoom_range=0.05,\n",
        "        #width_shift_range=0.05,\n",
        "        #height_shift_range=0.05,\n",
        "        #shear_range=0.05,\n",
        "        #horizontal_flip=True,\n",
        "        #fill_mode=\"nearest\",\n",
        "        #validation_split=0.20)\n",
        "\n",
        "#test_datagen = ImageDataGenerator(rescale=1 / 255.0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "46bfae0556cc79fc827cf62550d4b72d149cdfc64787c68439fc9ea0697c8cac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
